{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e34a32a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM, TimeDistributed, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "957a87c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>through</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>London</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-geo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>protest</td>\n",
       "      <td>VB</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>war</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>in</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Iraq</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-geo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>and</td>\n",
       "      <td>CC</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NaN</td>\n",
       "      <td>demand</td>\n",
       "      <td>VB</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>withdrawal</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NaN</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>NaN</td>\n",
       "      <td>British</td>\n",
       "      <td>JJ</td>\n",
       "      <td>B-gpe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>troops</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>from</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>NaN</td>\n",
       "      <td>that</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>NaN</td>\n",
       "      <td>country</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>NaN</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>Families</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Sentence #           Word  POS    Tag\n",
       "0   Sentence: 1      Thousands  NNS      O\n",
       "1           NaN             of   IN      O\n",
       "2           NaN  demonstrators  NNS      O\n",
       "3           NaN           have  VBP      O\n",
       "4           NaN        marched  VBN      O\n",
       "5           NaN        through   IN      O\n",
       "6           NaN         London  NNP  B-geo\n",
       "7           NaN             to   TO      O\n",
       "8           NaN        protest   VB      O\n",
       "9           NaN            the   DT      O\n",
       "10          NaN            war   NN      O\n",
       "11          NaN             in   IN      O\n",
       "12          NaN           Iraq  NNP  B-geo\n",
       "13          NaN            and   CC      O\n",
       "14          NaN         demand   VB      O\n",
       "15          NaN            the   DT      O\n",
       "16          NaN     withdrawal   NN      O\n",
       "17          NaN             of   IN      O\n",
       "18          NaN        British   JJ  B-gpe\n",
       "19          NaN         troops  NNS      O\n",
       "20          NaN           from   IN      O\n",
       "21          NaN           that   DT      O\n",
       "22          NaN        country   NN      O\n",
       "23          NaN              .    .      O\n",
       "24  Sentence: 2       Families  NNS      O"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data from CSV file -> got this dataset from kaggle\n",
    "data = pd.read_csv('../data/ner_dataset.csv', encoding=\"latin1\")\n",
    "# Take a quick look at the data -> notice that we have NaN for every sentence # except the first word in the sentence  \n",
    "data.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "59cbe3a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8x/9m1j4l7s593g3365mw4qsq2r0000gn/T/ipykernel_59548/1807906978.py:4: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for index, sentence_num in data['Sentence #'].iteritems():\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>country</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2</td>\n",
       "      <td>Families</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2</td>\n",
       "      <td>soldiers</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2</td>\n",
       "      <td>killed</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentence #      Word  POS Tag\n",
       "22          1   country   NN   O\n",
       "23          1         .    .   O\n",
       "24          2  Families  NNS   O\n",
       "25          2        of   IN   O\n",
       "26          2  soldiers  NNS   O\n",
       "27          2    killed  VBN   O"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes\n",
    "curr_sentence = 1\n",
    "# Iterate through the \"Sentence #\" column using .iteritems()\n",
    "for index, sentence_num in data['Sentence #'].iteritems():\n",
    "    if (pd.isna(sentence_num)):\n",
    "        data.loc[index, 'Sentence #'] = curr_sentence # Replace NaN with correct sentence number\n",
    "    else:\n",
    "        curr_sentence = int(sentence_num.split(': ')[1])\n",
    "        data.loc[index, 'Sentence #'] = curr_sentence # Replace Sentence: X with just X \n",
    "data.iloc[22:28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a6d1121a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentence #     int64\n",
       "Word          object\n",
       "POS           object\n",
       "Tag           object\n",
       "dtype: object"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Sentence #'] = data['Sentence #'].astype('int64') # We want to make the sentence# column integers\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "21c0b1d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Thousands', 16],\n",
       " ['of', 16],\n",
       " ['demonstrators', 16],\n",
       " ['have', 16],\n",
       " ['marched', 16],\n",
       " ['through', 16],\n",
       " ['London', 2],\n",
       " ['to', 16],\n",
       " ['protest', 16],\n",
       " ['the', 16],\n",
       " ['war', 16],\n",
       " ['in', 16],\n",
       " ['Iraq', 2],\n",
       " ['and', 16],\n",
       " ['demand', 16],\n",
       " ['the', 16],\n",
       " ['withdrawal', 16],\n",
       " ['of', 16],\n",
       " ['British', 3],\n",
       " ['troops', 16],\n",
       " ['from', 16],\n",
       " ['that', 16],\n",
       " ['country', 16],\n",
       " ['.', 16]]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_encoder = LabelEncoder()\n",
    "data['Tag_index'] = tag_encoder.fit_transform(data['Tag'])\n",
    "\n",
    "# Group data by sentence and create sequences of words and tags\n",
    "grouped_data = data.groupby('Sentence #')[['Word', 'Tag_index']].apply(lambda x: x.values.tolist()).to_dict()\n",
    "grouped_data[1] # We get a dictionary mapping each sentence to a list of pairs containing a given word + its tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5a1e2921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Thousands', 'of', 'demonstrators', 'have', 'marched', 'through', 'London', 'to', 'protest', 'the', 'war', 'in', 'Iraq', 'and', 'demand', 'the', 'withdrawal', 'of', 'British', 'troops', 'from', 'that', 'country', '.']\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "[16, 16, 16, 16, 16, 16, 2, 16, 16, 16, 16, 16, 2, 16, 16, 16, 16, 16, 3, 16, 16, 16, 16, 16]\n"
     ]
    }
   ],
   "source": [
    "# Convert grouped data into sequences of sentences and labels\n",
    "sentences = []\n",
    "labels = []\n",
    "for sentence, group in grouped_data.items():\n",
    "    words = [item[0] for item in group]\n",
    "    tags = [item[1] for item in group]\n",
    "    sentences.append(words)\n",
    "    labels.append(tags)\n",
    "# Here, we produce a list of words for each sentence & a list of tags for each sentence\n",
    "print (sentences[0])\n",
    "print (\"----------------------------------------------------------------------------------------------------------\")\n",
    "print (labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c4e8f406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an index for each unique word. Then create an index for unknown words\n",
    "word_to_index = {}\n",
    "for sentence in sentences:\n",
    "    for word in sentence:\n",
    "        if word not in word_to_index:\n",
    "            word_to_index[word] = len(word_to_index)\n",
    "word_to_index['UNK'] = len(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "adcf76cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'B-art', 1: 'B-eve', 2: 'B-geo', 3: 'B-gpe', 4: 'B-nat', 5: 'B-org', 6: 'B-per', 7: 'B-tim', 8: 'I-art', 9: 'I-eve', 10: 'I-geo', 11: 'I-gpe', 12: 'I-nat', 13: 'I-org', 14: 'I-per', 15: 'I-tim', 16: 'O'}\n"
     ]
    }
   ],
   "source": [
    "index_to_tag = {idx: tag for idx, tag in enumerate(tag_encoder.classes_)}\n",
    "print (index_to_tag) # map between tags and their indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d4525909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 9, 15, 1, 16, 17, 18, 19, 20, 21]\n"
     ]
    }
   ],
   "source": [
    "sentences_indices = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    sentence_indices = [word_to_index.get(word, word_to_index['UNK']) for word in sentence]\n",
    "    sentences_indices.append(sentence_indices)\n",
    "print (sentences_indices[0]) # Same as sentences, but we use index for each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4b89428a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAX SEQUENCE LENGTH = 104\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(index_to_tag)\n",
    "\n",
    "# Here, we want to pad the x (word indices) & y (tag indices) data so that each is the same length\n",
    "# We'll cap out our sentences at whatever the word length of the longest sentence is\n",
    "# Find the maximum sequence length for both X and y\n",
    "max_sequence_length = max(max(len(sequence) for sequence in sentences_indices), max(len(sequence) for sequence in labels))\n",
    "print (\"MAX SEQUENCE LENGTH = \" + str(max_sequence_length))\n",
    "\n",
    "# Pad the word index sequences (x)\n",
    "x = pad_sequences(sentences_indices, maxlen=max_sequence_length, padding='post', value=0)  # Assuming '0' is the padding value\n",
    "\n",
    "# Pad the tag index sequences (y)\n",
    "y = pad_sequences(labels, maxlen=max_sequence_length, padding='post', value=0)  # Assuming '0' is the padding value\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "81b8dbee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14  9 15  1 16 17 18 19 20 21\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0]\n",
      "[16 16 16 16 16 16  2 16 16 16 16 16  2 16 16 16 16 16  3 16 16 16 16 16\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "# Sequences have extra zeroes at the end to normalize length\n",
    "print(x[0])\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6fdaa0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1199/1199 [==============================] - 56s 45ms/step - loss: 0.1135 - accuracy: 0.9748 - val_loss: 0.0305 - val_accuracy: 0.9914\n",
      "Epoch 2/5\n",
      "1199/1199 [==============================] - 56s 46ms/step - loss: 0.0228 - accuracy: 0.9934 - val_loss: 0.0241 - val_accuracy: 0.9928\n",
      "Epoch 3/5\n",
      "1199/1199 [==============================] - 56s 46ms/step - loss: 0.0160 - accuracy: 0.9951 - val_loss: 0.0231 - val_accuracy: 0.9931\n",
      "Epoch 4/5\n",
      "1199/1199 [==============================] - 55s 46ms/step - loss: 0.0128 - accuracy: 0.9960 - val_loss: 0.0238 - val_accuracy: 0.9930\n",
      "Epoch 5/5\n",
      "1199/1199 [==============================] - 59s 49ms/step - loss: 0.0105 - accuracy: 0.9966 - val_loss: 0.0247 - val_accuracy: 0.9931\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8d58f988e0>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define params\n",
    "embedding_dim = 100\n",
    "lstm_units = 64     \n",
    "num_epochs = 5 # This ended up being well enough    \n",
    "batch_size = 32\n",
    "num_words = len(word_to_index)\n",
    "num_tags = 17     \n",
    "\n",
    "# Build bidirectional LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=num_words, output_dim=embedding_dim, input_length=max_sequence_length))\n",
    "model.add(Bidirectional(LSTM(units=lstm_units, return_sequences=True)))\n",
    "model.add(TimeDistributed(Dense(units=num_tags, activation='softmax'))) # softmax for multiple categories\n",
    "\n",
    "# We didn't use to_categorical() on the y data so we can use sparse instead of normal categorical crossentropy\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "model.fit(x_train, y_train, epochs=num_epochs, batch_size=batch_size, validation_data=(x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "88b9d90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on test data\n",
      "75/75 [==============================] - 3s 33ms/step - loss: 0.0247 - accuracy: 0.9931\n",
      "test loss: 0.02473883517086506\n",
      "test accuracy: 0.9931333065032959\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluate on test data\")\n",
    "results = model.evaluate(x_test, y_test, batch_size=128)\n",
    "print(\"test loss: \" + str(results[0]))\n",
    "print(\"test accuracy: \" + str(results[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1d6f13fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split sentence:\n",
      "['Millions', 'of', 'people', 'gathered', 'in', 'my', 'home', 'state', 'of', 'Massachusetts', 'because', 'Jim', 'was', 'there.']\n",
      "1/1 [==============================] - 0s 370ms/step\n",
      "Predictions:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'B-per', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "# Test with example sentence\n",
    "example_sentence = \"Millions of people gathered in my home state of Massachusetts because Jim was there.\"\n",
    "\n",
    "# Tokenize sentence, convert to word indices, & pad\n",
    "words = example_sentence.split()\n",
    "print(\"Split sentence:\")\n",
    "print(words)\n",
    "sentence_length = len(words)\n",
    "word_indices = [word_to_index.get(word, word_to_index['UNK']) for word in words]\n",
    "padded_sequence = pad_sequences([word_indices], maxlen=max_sequence_length, padding='post', value=0)\n",
    "\n",
    "# Make predictions\n",
    "example_predictions = model.predict(padded_sequence)\n",
    "\n",
    "# Print predictions\n",
    "ex_tags = []\n",
    "print(\"Predictions:\")\n",
    "for tag_probabilities in example_predictions[0]:\n",
    "    ex_tags.append(index_to_tag[np.argmax(tag_probabilities)])\n",
    "print (ex_tags[:sentence_length])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "15c23b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the model\n",
    "model.save('ner_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7a199b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_str = repr(word_to_index)\n",
    "\n",
    "# Save the dictionary to a separate script\n",
    "with open(\"../app/ner_variable_storage.py\", \"w\") as file:\n",
    "    file.write(\"max_sequence_length = \" + str(max_sequence_length) + \"\\n\")\n",
    "    file.write(\"word_to_index = \" + dict_str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
